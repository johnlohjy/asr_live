{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define ASR Model and dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import time\n",
    "\n",
    "# Model setup code for distil-whisper small\n",
    "model_id = \"distil-whisper/distil-small.en\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Define dataset for testing\n",
    "dataset_repo = \"johnlohjy/imda_nsc_p3_same_closemic_train\"\n",
    "dataset = load_dataset(dataset_repo, split='train', streaming=True, trust_remote_code=True)\n",
    "dataset_iter = iter(dataset)\n",
    "sample = next(dataset_iter)\n",
    "sample = sample[\"audio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Server-Related Classes, Adapted for Colab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import wave\n",
    "import threading\n",
    "\n",
    "class Client:\n",
    "    def __init__(self):\n",
    "        self.frames_np = None # To store frames buffer as a numpy array\n",
    "        self.timestamp_offset = 0.0 # Track transcription offset from the very start\n",
    "        self.frames_offset = 0.0 # Track frames offset from the very start/Duration of audio discarded\n",
    "        self.send_last_n_segments = 10  # Number of transcribed segments that will be 'sent' to the client\n",
    "        # self.lock = threading.Lock() # for shared resources        \n",
    "        # SINGLE_MODEL_LOCK = threading.Lock() # Check whats this \n",
    "        self.eos # End-Of-Speech Flag\n",
    "        self.transcriber = None # Initialize with Whisper ASR model \n",
    "\n",
    "        # Thread to run speech-to-text function\n",
    "        self.trans_thread = threading.Thread(target=self.speech_to_text)\n",
    "        self.trans_thread.start()\n",
    "\n",
    "    def add_frames(self, frame_np):\n",
    "        '''\n",
    "        Add new audio chunks to frames buffer\n",
    "\n",
    "        Check if need lock to implement lock\n",
    "        '''\n",
    "        # Manage the ongoing buffer\n",
    "        # If the buffer is more than 45s\n",
    "        if self.frames_np is not None and self.frames_np.shape[0] > 45*self.RATE:\n",
    "            # Increase frames_offset by 30s\n",
    "            # Discard oldest 30s of audio from buffer\n",
    "            self.frames_offset += 30.0\n",
    "            self.frames_np = self.frames_np[int(30*self.RATE):]\n",
    "            \n",
    "            # Update timestamp_offset\n",
    "            if self.timestamp_offset < self.frames_offset:\n",
    "                self.timestamp_offset = self.frames_offset\n",
    "\n",
    "        # If the frame buffer is empty, initialise it with the new audio frames\n",
    "        if self.frames_np is None:\n",
    "            self.frames_np = frame_np.copy()\n",
    "        # Else, append the new audio chunk to the existing buffer\n",
    "        else:\n",
    "            self.frames_np = np.concatenate((self.frames_np, frame_np), axis=0)\n",
    "\n",
    "    def save_frames(self):\n",
    "        '''\n",
    "        Sample code to save the audio when client disconnects\n",
    "        '''\n",
    "        fp = os.path.join(os.getcwd(), \"test_frames.wav\")\n",
    "        with wave.open(fp, \"wb\") as wavfile:\n",
    "            wavfile: wave.Wave_write\n",
    "            wavfile.setnchannels(1)\n",
    "            wavfile.setsampwidth(2)\n",
    "            wavfile.setframerate(16000)\n",
    "            wavfile.writeframes(self.frames_np)\n",
    "    \n",
    "    def clip_audio_if_no_valid_segment(self):\n",
    "        '''\n",
    "        If there is > 25s of audio to transcribe in the buffer,\n",
    "        adjust timestamp_offset such that its only 5s behind the total audio added so far\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def get_audio_chunk_for_processing(self):\n",
    "        pass \n",
    "\n",
    "    def transcribe_audio(self):\n",
    "        pass\n",
    "\n",
    "    def speech_to_text(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ClientManager:\n",
    "    '''\n",
    "    Custom client manager class to handle clients connected over the \n",
    "    WebSocket server\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.clients = {}\n",
    "\n",
    "    def add_client(self, websocket, client):\n",
    "        '''\n",
    "        Add a WebSocket server connection info and its associated client\n",
    "        '''\n",
    "        self.clients[websocket] = client\n",
    "\n",
    "    def get_client(self, websocket):\n",
    "        '''\n",
    "        Retrieve a client associated with the WebSocket server connection info provided\n",
    "        '''\n",
    "        if websocket in self.clients:\n",
    "            return self.clients[websocket]\n",
    "        return False \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Server:\n",
    "    '''\n",
    "    Server class handles\n",
    "    - New client connections\n",
    "    - Receiving and processing audio from client\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.client_manager = None\n",
    "        self.vad_detector = None #todo: VAD detector\n",
    "        self.no_voice_activity_chunks = 0 # Help to track prolonged silence\n",
    "\n",
    "    def initialize_client(self, websocket):\n",
    "        '''\n",
    "        Initialize the new client and add it to the client manager\n",
    "\n",
    "        EXPAND ON CLIENT CLASS (see ServeClientTensorRT, ServeClientBase)\n",
    "        '''\n",
    "        # Initialize the client\n",
    "        client = Client()\n",
    "\n",
    "        # Add the client to the client manager\n",
    "        self.client_manager.add_client(websocket, client)\n",
    "\n",
    "    def handle_new_connection(self,websocket):\n",
    "        '''\n",
    "        Initialise the client manager\n",
    "        Initialise the new client and add it to the client manager\n",
    "        '''\n",
    "\n",
    "        # Initialise the client manager if not done\n",
    "        if self.client_manager is None:\n",
    "            self.client_manager = ClientManager()\n",
    "\n",
    "        # todo: Use a Voice Activity Detector\n",
    "        # self.vad_detector = VoiceActivityDetector()\n",
    "\n",
    "        # Initialise the new client and add it to the client manager\n",
    "        self.initialize_client(websocket)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def get_audio_from_websocket(self, websocket):\n",
    "        '''\n",
    "        Receive audio chunks from the WebSocket and create a numpy array out of it\n",
    "        '''\n",
    "        # Subsequently, receive audio data (message) over the WebSocket server connection\n",
    "        # https://websockets.readthedocs.io/en/stable/reference/sync/server.html#websockets.sync.server.ServerConnection.recv\n",
    "        frame_data = websocket.recv()\n",
    "\n",
    "        # Creates numpy array without copying it (more efficient)\n",
    "        return np.frombuffer(frame_data, dtype=np.float32)\n",
    "\n",
    "    def voice_activity(self, websocket, frame_np):\n",
    "        '''\n",
    "        todo: Add in websocket argument\n",
    "        threshold config for no voice activity chunks\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        Whenever no voice activity is detected, increment the counter no_voice_activity_chunks\n",
    "        \n",
    "        If the counter is > 3 i.e. prolonged silence: \n",
    "        - set the end of speech flag of the client to True \n",
    "        - wait for .1 seconds -> ??? is this needed\n",
    "        - return False for no voice activity\n",
    "        return True for voice activity\n",
    "        '''\n",
    "        if not self.vad_detector(frame_np):\n",
    "            self.no_voice_activity_chunks += 1\n",
    "            if self.no_voice_activity_chunks > 3:\n",
    "                client = self.client_manager.get_client(websocket)\n",
    "                if not client.eos:\n",
    "                    client.set_eos(True)\n",
    "                time.sleep(0.1)    # Sleep 100m; wait some voice activity. todo-> check if this needed\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def process_audio_frames(self, websocket):\n",
    "        '''\n",
    "        Get the audio chunk from the WebSocket as a numpy array\n",
    "\n",
    "        Send a dummy transcription back to the client first\n",
    "\n",
    "        TOCHECK: VAD\n",
    "        '''\n",
    "        # Get the audio chunk from the WebSocket as a numpy array\n",
    "        # todo: Change to receive from audio file in a loop to be used in colab\n",
    "        frame_np = self.get_audio_from_websocket(websocket)\n",
    "        \n",
    "        # Get the client using its associated WebSocket\n",
    "        # todo: Change key to be used in colab\n",
    "        client = self.client_manager.get_client(websocket)\n",
    "        \n",
    "        # Check for voice activity in the audio chunk\n",
    "        # If there is no voice activity return False\n",
    "        # If there is prolonged silence, set the eos flag of the client to False\n",
    "        # If there is voice activity return True\n",
    "        voice_active = self.voice_activity(websocket, frame_np)\n",
    "\n",
    "        # If there is voice activity, reset the\n",
    "        # - no_voice_activity_chunks\n",
    "        # - eos flag\n",
    "        if voice_active:\n",
    "            self.no_voice_activity_chunks = 0\n",
    "            client.set_eos(False)\n",
    "        # If there is no voice activity, return True\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "        # If there is voice activity,\n",
    "        # add frames to the client's buffer and return True\n",
    "        client.add_frames(frame_np)\n",
    "        return True\n",
    "\n",
    "    def recv_audio(self,websocket):\n",
    "        \"\"\"\n",
    "        First handle the new connection\n",
    "\n",
    "        Continously process audio frames\n",
    "        \"\"\"\n",
    "\n",
    "        # Try to handle the new connection\n",
    "        if not self.handle_new_connection(websocket):\n",
    "            return\n",
    "        \n",
    "        # Continously process audio frames\n",
    "        while True: \n",
    "            if not self.process_audio_frames(websocket):\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loop to simulate sending server audio data (from file) and Server sending client transcription (print)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
