{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define ASR Model and dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import time\n",
    "\n",
    "# Model setup code for distil-whisper small\n",
    "model_id = \"distil-whisper/distil-small.en\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Define dataset for testing\n",
    "dataset_repo = \"johnlohjy/imda_nsc_p3_same_closemic_train\"\n",
    "dataset = load_dataset(dataset_repo, split='train', streaming=True, trust_remote_code=True)\n",
    "dataset_iter = iter(dataset)\n",
    "sample = next(dataset_iter)\n",
    "sample = sample[\"audio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define VAD model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "vad.py from WhisperLive similar to utils_vad.py from sliero-vad\n",
    "https://github.com/snakers4/silero-vad. See vad.py from WhisperLive\n",
    "https://github.com/snakers4/silero-vad/discussions/201\n",
    "https://www.reddit.com/r/MachineLearning/comments/rj67dz/p_silero_vad_one_voice_detector_to_rule_them_all/?rdt=48974\n",
    "\n",
    "\n",
    "\n",
    "utils_vad.py\n",
    "threshold: float (default - 0.5)\n",
    "Speech threshold. Silero VAD outputs speech probabilities for each audio chunk, probabilities ABOVE this value are considered as SPEECH.\n",
    "It is better to tune this parameter for each dataset separately, but \"lazy\" 0.5 is pretty good for most datasets\n",
    "\n",
    "neg_threshold: float (default = threshold - 0.15)\n",
    "Negative threshold (noise or exit threshold). If model's current state is SPEECH, values BELOW this value are considered as NON-SPEECH.\n",
    "\n",
    "\n",
    "for current_start_sample in range(0, audio_length_samples, window_size_samples):\n",
    "    chunk = audio[current_start_sample: current_start_sample + window_size_samples]\n",
    "    if len(chunk) < window_size_samples:\n",
    "        chunk = torch.nn.functional.pad(chunk, (0, int(window_size_samples - len(chunk))))\n",
    "    speech_prob = model(chunk, sampling_rate).item()\n",
    "\n",
    "speech_prob >= threshold\n",
    "\n",
    "x is an audio chunk\n",
    "speech_prob = self.model(x, self.sampling_rate).item()\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Server-Related Classes, Adapted for Colab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import wave\n",
    "import threading\n",
    "import textwrap\n",
    "\n",
    "class Client:\n",
    "    def __init__(self):\n",
    "        self.frames_np = None # Store frames buffer as a numpy array\n",
    "        self.frames_offset = 0.0 # Track frames offset from the very start/Duration of audio discarded\n",
    "        self.timestamp_offset = 0.0 # Track transcription offset from the very start\n",
    "        self.send_last_n_segments = 10  # Number of last transcribed segments that will be 'sent' to the client \n",
    "        self.eos # End-Of-Speech Flag\n",
    "        self.transcriber = None # Initialize Whisper ASR model \n",
    "        self.transcript = [] # Store fully transcribed segments\n",
    "\n",
    "        self.lock = threading.Lock() # for shared resources: frames_np. https://realpython.com/python-thread-lock/#threadinglock-for-primitive-locking\n",
    "        # SINGLE_MODEL = None # only necessary for multiple clients?\n",
    "        # SINGLE_MODEL_LOCK = threading.Lock() # only necessary for multiple clients?\n",
    "\n",
    "        # Thread to run speech-to-text function\n",
    "        self.trans_thread = threading.Thread(target=self.speech_to_text)\n",
    "        self.trans_thread.start()\n",
    "\n",
    "\n",
    "    def add_frames(self, frame_np):\n",
    "        '''\n",
    "        Manage the ongoing buffer's size\n",
    "        Add new audio chunks to the client's frames buffer\n",
    "        '''\n",
    "        # Manage the ongoing buffer\n",
    "        # Lock required as frames_np, frames_offset, timestamp_offset is a shared resource with the speech_to_text thread code\n",
    "        # lock the critical section of code below (lock shared resources, no 2 threads can modify)\n",
    "        self.lock.acquire()\n",
    "        # If the buffer is more than 45s\n",
    "        if self.frames_np is not None and self.frames_np.shape[0] > 45*self.RATE:\n",
    "            # Increase frames_offset by 30s\n",
    "            # Discard oldest 30s of audio from buffer\n",
    "            self.frames_offset += 30.0\n",
    "            self.frames_np = self.frames_np[int(30*self.RATE):]\n",
    "            # Update timestamp_offset\n",
    "            if self.timestamp_offset < self.frames_offset:\n",
    "                self.timestamp_offset = self.frames_offset\n",
    "\n",
    "        # If the frame buffer is empty, initialise it with the new audio frames\n",
    "        if self.frames_np is None:\n",
    "            self.frames_np = frame_np.copy()\n",
    "        # Else, append the new audio chunk to the existing buffer\n",
    "        else:\n",
    "            self.frames_np = np.concatenate((self.frames_np, frame_np), axis=0)\n",
    "        # unlock the critical section of code above\n",
    "        self.lock.release()\n",
    "\n",
    "    \n",
    "    def speech_to_text(self):\n",
    "        '''\n",
    "        Process audio buffer in an infinite loop, continuously transcribing speech\n",
    "        '''\n",
    "        while True:\n",
    "            # Wait for some chunks to arrive\n",
    "            if self.frames_np is None:\n",
    "                time.sleep(0.02)\n",
    "                continue\n",
    "\n",
    "            # Adjust the timestamp_offset \n",
    "            self.clip_audio_if_no_valid_segment()\n",
    "\n",
    "            # Get the audio to be transcribed using the timestamp_offset\n",
    "            input_bytes, duration = self.get_audio_chunk_for_processing()\n",
    "            if duration < 0.4:\n",
    "                continue\n",
    "\n",
    "            # Transcribe the audio and \n",
    "            input_sample = input_bytes.copy()\n",
    "            self.transcribe_audio(input_sample)\n",
    "\n",
    "    \n",
    "    def clip_audio_if_no_valid_segment(self):\n",
    "        '''\n",
    "        If there is > 25s of audio to transcribe in the buffer,\n",
    "        adjust timestamp_offset such that its only 5s behind the total audio added so far\n",
    "\n",
    "        If there is <=25s of audio to transcribe, its okay\n",
    "        '''\n",
    "        # Lock is required because frames_np, frames_offset, timestamp_offset is a shared resource\n",
    "        # Written as a context manager -> Auto acquires and release lock\n",
    "        with self.lock:\n",
    "            # If there is more than 25s of audio to transcribe\n",
    "            if self.frames_np[int((self.timestamp_offset - self.frames_offset)*self.RATE):].shape[0] > 25 * self.RATE:\n",
    "                # Adjust timestamp_offset s.t it is only 5s behind the total audio added so far\n",
    "                duration = self.frames_np.shape[0] / self.RATE\n",
    "                self.timestamp_offset = self.frames_offset + duration - 5\n",
    "\n",
    "\n",
    "    def get_audio_chunk_for_processing(self):\n",
    "        '''\n",
    "        Get the audio to be transcribed from the buffer calculated using timestamp_offset\n",
    "        '''\n",
    "        # Use timestamp_offset to help subset the buffer to get the audio to be transcribed\n",
    "        # Lock is required because frames_np, frames_offset, timestamp_offset is a shared resource\n",
    "        # Written as a context manager -> Auto acquires and release lock\n",
    "        with self.lock:\n",
    "            samples_take = max(0, (self.timestamp_offset - self.frames_offset) * self.RATE)\n",
    "            input_bytes = self.frames_np[int(samples_take):].copy()\n",
    "        duration = input_bytes.shape[0] / self.RATE\n",
    "        return input_bytes, duration\n",
    "    \n",
    "\n",
    "    def transcribe_audio(self, input_bytes, duration):\n",
    "        '''\n",
    "        Transcribe the audio to be transcribed\n",
    "        '''\n",
    "        # todo: check how to use transcription model\n",
    "        # Transcribe the audio to be transcribed\n",
    "        last_segment = self.transcriber.transcribe(input_bytes)\n",
    "        # Print the transcriptions to be printed and \n",
    "        # if there is prolonged silence, \n",
    "        # - Save the latest speech segment\n",
    "        # - Update the timestamp_offset\n",
    "        self.handle_transcription_output(last_segment, duration)\n",
    "\n",
    "\n",
    "    def handle_transcription_output(self, last_segment, duration):\n",
    "        '''\n",
    "        Print the transcriptions to be printed\n",
    "        If the client's end-of-speech flag is True \n",
    "        (prolonged silence, means that speech segment is finalized), \n",
    "        - Save the latest speech segment\n",
    "        - Update the timestamp_offset\n",
    "        '''\n",
    "        segments = self.prepare_segments({\"text\": last_segment})\n",
    "        self.send_transcription_to_client(segments)\n",
    "        if self.eos:\n",
    "            self.update_timestamp_offset(last_segment, duration)\n",
    "\n",
    "\n",
    "    def prepare_segments(self, last_segment=None):\n",
    "        '''\n",
    "        Prepare the segments to be printed\n",
    "        '''\n",
    "        segments = []\n",
    "        # If the length of self.transcript is more than or equal send_last_n_segments, \n",
    "        # set segments to be the last send_last_n_segments number of elements of self.transcript\n",
    "        if len(self.transcript) >= self.send_last_n_segments:\n",
    "            segments = self.transcript[-self.send_last_n_segments:].copy()\n",
    "        # If not, just set segments to be self.transcript\n",
    "        else:\n",
    "            segments = self.transcript.copy()\n",
    "        # If the segment that was just transcribed is not None, add it to segments\n",
    "        if last_segment is not None:\n",
    "            segments = segments + [last_segment]\n",
    "        return segments\n",
    "    \n",
    "\n",
    "    def send_transcription_to_client(self, text):\n",
    "        '''\n",
    "        Print the transcription for testing on Colab\n",
    "        '''\n",
    "        wrapper = textwrap.TextWrapper(width=60)\n",
    "        for line in wrapper.wrap(text=\"\".join(text)):\n",
    "            print(line)\n",
    "\n",
    "\n",
    "    def update_timestamp_offset(self, last_segment, duration):\n",
    "        # If self.transcript is empty, add the last segment received\n",
    "        if not len(self.transcript):\n",
    "            self.transcript.append({\"text\": last_segment + \" \"})\n",
    "        # If the last element of self.transcript is != to the last segment, add the last segment\n",
    "        elif self.transcript[-1][\"text\"].strip() != last_segment:\n",
    "            self.transcript.append({\"text\": last_segment + \" \"})\n",
    "        # Lock is required because frames_np, frames_offset, timestamp_offset is a shared resource\n",
    "        # Written as a context manager -> Auto acquires and release lock\n",
    "        # Update the timestamp_offset as this portion of the buffer has been finalized \n",
    "        # (doesn't need to be transcribed anymore)\n",
    "        with self.lock:\n",
    "            self.timestamp_offset += duration\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "class ClientManager:\n",
    "    '''\n",
    "    Custom client manager class to handle clients connected over the \n",
    "    WebSocket server\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.clients = {}\n",
    "\n",
    "    def add_client(self, websocket, client):\n",
    "        '''\n",
    "        Add a WebSocket server connection info and its associated client\n",
    "        '''\n",
    "        self.clients[websocket] = client\n",
    "\n",
    "    def get_client(self, websocket):\n",
    "        '''\n",
    "        Retrieve a client associated with the WebSocket server connection info provided\n",
    "        '''\n",
    "        if websocket in self.clients:\n",
    "            return self.clients[websocket]\n",
    "        return False \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Server:\n",
    "    def __init__(self):\n",
    "        self.client_manager = None\n",
    "        self.vad_detector = None # todo: VAD detector\n",
    "        self.no_voice_activity_chunks = 0 # Help to track prolonged silence\n",
    "\n",
    "    def recv_audio(self,websocket):\n",
    "        \"\"\"\n",
    "        Handle the new connection\n",
    "        Continously process audio frames\n",
    "\n",
    "        todo: when writing the loop, pass \"websocket\" as the websocket argument\n",
    "        \"\"\"\n",
    "        # Handle the new connection\n",
    "        if not self.handle_new_connection(websocket):\n",
    "            return\n",
    "        \n",
    "        # Continously process audio frames\n",
    "        while True: \n",
    "            if not self.process_audio_frames(websocket):\n",
    "                break\n",
    "\n",
    "\n",
    "    def handle_new_connection(self,websocket):\n",
    "        '''\n",
    "        Initialise the client manager\n",
    "        Set the VAD\n",
    "        Initialise the new client and add it to the client manager\n",
    "        '''\n",
    "\n",
    "        # Initialise the client manager if not done\n",
    "        if self.client_manager is None:\n",
    "            self.client_manager = ClientManager()\n",
    "\n",
    "        # todo: Set the VAD\n",
    "        # self.vad_detector = VoiceActivityDetector()\n",
    "\n",
    "        # Initialise the new client and add it to the client manager\n",
    "        # todo: Change websocket key\n",
    "        self.initialize_client(websocket)\n",
    "\n",
    "        return True\n",
    "    \n",
    "\n",
    "    def initialize_client(self, websocket):\n",
    "        '''\n",
    "        Initialize the new client and add it to the client manager\n",
    "        '''\n",
    "        # Initialize the new client\n",
    "        client = Client()\n",
    "\n",
    "        # Add the client to the client manager\n",
    "        self.client_manager.add_client(websocket, client)\n",
    "\n",
    "    \n",
    "    def process_audio_frames(self, websocket):\n",
    "        '''\n",
    "        Get the audio chunk from the WebSocket\n",
    "        If it has voice activity\n",
    "        - Reset the no voice activity settings\n",
    "        - Add the audio chunk to the client's buffer\n",
    "        If it has no voice activity\n",
    "        - return True\n",
    "        '''\n",
    "        # Get the audio chunk from the WebSocket as a numpy array\n",
    "        # todo: Change websocket arg to receive from audio file in a loop to be used in colab\n",
    "        frame_np = self.get_audio_from_websocket(websocket)\n",
    "        \n",
    "        # Get the client using its associated WebSocket\n",
    "        # todo: Change key used in colab\n",
    "        client = self.client_manager.get_client(websocket)\n",
    "        \n",
    "        # Check for voice activity in the audio chunk\n",
    "        # - if there is no voice activity return False\n",
    "        # - if there is prolonged silence (accumulated), set the eos flag of the client to True\n",
    "        # - if there is voice activity return True\n",
    "        voice_active = self.voice_activity(websocket, frame_np)\n",
    "\n",
    "        # If there is voice activity, reset the\n",
    "        # - no_voice_activity_chunks\n",
    "        # - eos flag\n",
    "        # - add audio chunk to the client's buffer \n",
    "        if voice_active:\n",
    "            self.no_voice_activity_chunks = 0\n",
    "            client.set_eos(False)\n",
    "            client.add_frames(frame_np)\n",
    "        return True\n",
    "\n",
    "\n",
    "    def get_audio_from_websocket(self, websocket):\n",
    "        '''\n",
    "        Receive audio chunks from the WebSocket and create a numpy array out of it\n",
    "        '''\n",
    "        # Subsequently, receive audio data (message) over the WebSocket server connection\n",
    "        # todo: change the way audio chunk is received for use in colab\n",
    "        frame_data = websocket.recv()\n",
    "        # Creates numpy array without copying it (more efficient)\n",
    "        return np.frombuffer(frame_data, dtype=np.float32)\n",
    "\n",
    "\n",
    "    def voice_activity(self, websocket, frame_np):\n",
    "        '''\n",
    "        todo\n",
    "        - Add in threshold config for no voice activity chunks\n",
    "\n",
    "        Whenever no voice activity is detected, increment no_voice_activity_chunks\n",
    "        \n",
    "        If the counter is > 3 i.e. prolonged silence: \n",
    "        - set the end-of-speech flag of the client to True \n",
    "        - wait for .1 seconds. todo: is this needed\n",
    "        \n",
    "        return False for no voice activity\n",
    "        \n",
    "        return True for voice activity\n",
    "        '''\n",
    "        if not self.vad_detector(frame_np):\n",
    "            self.no_voice_activity_chunks += 1\n",
    "            if self.no_voice_activity_chunks > 3:\n",
    "                client = self.client_manager.get_client(websocket)\n",
    "                if not client.eos:\n",
    "                    client.set_eos(True)\n",
    "                time.sleep(0.1)    # Sleep 100m; wait some voice activity\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loop to simulate sending server audio data (from file) and Server sending client transcription (print)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
