{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUmuqGKKOqeY"
      },
      "source": [
        "## **Evaluate Fine-Tuned Noise-Augmented Whisper Model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "g2mlgSaUMPMD",
        "outputId": "26834ec3-6149-424e-879a-147037c88c1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio triton nvidia-cublas-cu12 nvidia-cuda-runtime-cu12 nvidia-cudnn-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-nccl-cu12"
      ],
      "metadata": {
        "id": "72n-rIKVMPfm",
        "outputId": "932732c2-d237-45b9-9557-9da9de6ae823",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Found existing installation: triton 3.2.0\n",
            "Uninstalling triton-3.2.0:\n",
            "  Successfully uninstalled triton-3.2.0\n",
            "Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "  Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "  Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "  Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "  Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "  Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "  Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "  Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "  Successfully uninstalled nvidia-nccl-cu12-2.21.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu125"
      ],
      "metadata": {
        "id": "2ahvHesOMReS",
        "outputId": "0745c3ee-d01e-4d63-c6e3-3b853083b44a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu125\n",
            "Collecting torch\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.2.0 (from torch)\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0 triton-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "print(\"Torch Version:\", torch.__version__)\n",
        "print(\"Torch CUDA Version:\", torch.version.cuda)\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "print(\"Triton Version:\", triton.__version__)\n",
        "print(\"cuDNN Version:\", cudnn.version())"
      ],
      "metadata": {
        "id": "2-Bk7v5BMnot",
        "outputId": "5fa650e3-2b98-4fa7-9df2-e328a00ecae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch Version: 2.6.0+cu124\n",
            "Torch CUDA Version: 12.4\n",
            "CUDA Available: True\n",
            "Triton Version: 3.2.0\n",
            "cuDNN Version: 90100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate jiwer pydub bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "XWqBAt5xccjG",
        "outputId": "bcbef86d-42ac-4cee-af53-5c9248997e34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
        "from google.colab import drive\n",
        "import time\n",
        "import warnings\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from itertools import islice\n",
        "from torch.utils.data import IterableDataset\n",
        "import json\n",
        "import os\n",
        "import wave\n",
        "import threading\n",
        "import textwrap\n",
        "from IPython.display import clear_output\n",
        "import evaluate\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "import collections\n",
        "import pickle\n",
        "import random\n",
        "from pydub import AudioSegment\n",
        "from IPython.display import Audio\n",
        "import os\n",
        "import torch"
      ],
      "metadata": {
        "id": "8moRVO3vbDzD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Tell the progam to use the GPU allocated to us by setting the env variable used by CUDA\n",
        "# Use the first GPU on your machine\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "1cr0zAHIOz05"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive Env Setup\n",
        "whisper_ver = 'whisper-base'\n",
        "checkpoint_num = '2600'\n",
        "drive.mount('/content/drive')\n",
        "checkpoint_path = f'/content/drive/My Drive/{whisper_ver}-noiseaugmented-minieval-checkpoints/checkpoint-{checkpoint_num}'\n",
        "\n",
        "# Model setup code for fine-tuned whisper\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = WhisperForConditionalGeneration.from_pretrained(checkpoint_path).to(device)\n",
        "model.config.use_cache = True\n",
        "processor = WhisperProcessor.from_pretrained(f\"openai/{whisper_ver}\", language=\"en\", task=\"transcribe\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds45Ivg0aqL4",
        "outputId": "62adeefb-9756-47e8-d85d-317f5df14952"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive Env Setup\n",
        "whisper_ver = 'whisper-base'\n",
        "checkpoint_num = '4000'\n",
        "drive.mount('/content/drive')\n",
        "checkpoint_path = f'/content/drive/My Drive/{whisper_ver}-noiseaugmented-reverb-minieval-checkpoints/checkpoint-{checkpoint_num}'\n",
        "\n",
        "# Model setup code for fine-tuned whisper\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = WhisperForConditionalGeneration.from_pretrained(checkpoint_path).to(device)\n",
        "model.config.use_cache = True\n",
        "processor = WhisperProcessor.from_pretrained(f\"openai/{whisper_ver}\", language=\"en\", task=\"transcribe\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpkbEqpYJKA1",
        "outputId": "c1e6bda8-365d-4ee6-901c-ef5c1e7fa39a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "# Load long audio\n",
        "waveform, sr = torchaudio.load(\"./noisy_canteen.wav\")\n",
        "waveform = torchaudio.functional.resample(waveform, sr, 16000)\n",
        "start_sec = 6\n",
        "end_sec = 56\n",
        "start_sample = int(start_sec * 16000)\n",
        "end_sample = int(end_sec * 16000)\n",
        "# Crop the waveform\n",
        "waveform = waveform[:, start_sample:end_sample]\n",
        "waveform = waveform.mean(dim=0)  # Convert to mono if needed\n",
        "sr = 16000\n",
        "\n",
        "# Define chunk parameters\n",
        "chunk_duration = 15  # seconds\n",
        "chunk_samples = chunk_duration * sr\n",
        "\n",
        "# Split into chunks\n",
        "transcription = []\n",
        "for i in range(0, waveform.shape[0], chunk_samples):\n",
        "    chunk = waveform[i:i+chunk_samples]\n",
        "    input_features = processor(chunk.numpy(), sampling_rate=sr, return_tensors=\"pt\").input_features.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predicted_ids = model.generate(input_features)\n",
        "        text = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "        transcription.append(text)\n",
        "\n",
        "transcription = \" \".join(transcription)\n",
        "print(\"Full Transcription:\\n\", transcription)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e09b4M7WBzVJ",
        "outputId": "03477d0e-6618-4abf-eef0-28c1f5da687d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Transcription:\n",
            " you have been eh bro you reach already ah just reach the sun today damn strong sia i walk from bus stop already want to sweat like one dog ya lor i melt already melting already oh sweat wet ya lor got bring water or not oh but never refill eh useless one ah then let's just go refill ah later lecture confirm cannot find hard to refill oh ya later collector ah ya the govm one you pair already half half lah i read the slides but then the reading ah i give up already then ya got free some more i need to do the practice question  last time must do that or not wah you still got book do ah i straight away not heard after dinner oh dried sale i already blur from last week's lecture it's just a tutor call me and confirm blur just act busy can already lah don't down type something they won't disturb you what i tried that last time leh but still got nah wah jialat okay okay later i spend six five you later i help you start help me to start uh steady okay i change topic later\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"wer\")\n",
        "normalizer = BasicTextNormalizer()"
      ],
      "metadata": {
        "id": "tsGk-pGOfUGf"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = \"\"\"\n",
        "Eh bro you reach already ah Just reach The sun today damn strong sia I walk from bus stop only already sweat like one dog Ya lor I melting already Whole shirt wet liao You got bring water or not Got but never refill eh Useless one ah Then better go refill ah Later lecture confirm cannot tahan oh ya later got lecture ah Ya the two pm one You prepare already Half half lah I read the slides but then the reading ah I give up already Same sia Got quiz some more I anyhow do the practice questions last night but still cannot Wah you still got mood do ah I straightaway knock out after dinner No choice leh I already blur from last week’s lecture Later the tutor call me I confirm blur Just act busy can already lah Look down type something they won’t disturb you one I try that last time leh but still kena Wah jialat Okay okay later I sit beside you I help you if you stuck Wah steady Ok i treat you kopi later Kopi only I want\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SONJ3HNTCGdT"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_normalized = normalizer(transcription)\n",
        "reference_normalized = normalizer(reference)\n",
        "wer = 100 * metric.compute(predictions=[prediction_normalized], references=[reference_normalized])\n",
        "print(f'The offline WER is {wer}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpQ054zaCH9X",
        "outputId": "0a33f2d2-53fe-48a5-b853-df93cf71d3dc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The offline WER is 39.15343915343915%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y1yJuDwIEOtF"
      },
      "execution_count": 46,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}